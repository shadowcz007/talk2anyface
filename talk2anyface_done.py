# -*- coding: utf-8 -*-
"""talk2anyface-done.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cP2zAa6Q9EVnEeYiwrmkfc3BELbN0HK6
"""

#判断人脸
# !pip install face_recognition
# !pip install paddlepaddle-gpu==2.4.1.post116 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# #聊天机器人 
# !pip install --upgrade paddlenlp>=2.5.0 -i https://pypi.org/simple
# from paddlenlp import Taskflow
# dialogue = Taskflow("dialogue")
# dialogue(['你好呀'])

# help(paddlenlp)

# 文生图
# !pip install --upgrade ppdiffusers
#!pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html
# 安装包里没有FastDeployStableDiffusionPipeline
# from ppdiffusers import StableDiffusionPipeline
# # sd = FastDeployStableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5@fastdeploy",requires_safety_checker=False)
# sd = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
# text='girl'
# prompt = "a symmetrical portrait of "+text+". intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing "
# images = sd(prompt, guidance_scale=7.5,
#              num_inference_steps=20,
#              height=416, width=416).images

# image=images[0]
# image.save("test.png")
# print(image)

# 语音
# !pip install --upgrade pytest-runner 
# !pip install --upgrade paddlespeech

import base64
import librosa
import soundfile as sf

def resample_rate(path,new_sample_rate = 16000):
    signal, sr = librosa.load(path, sr=None)
    wavfile = path.split('/')[-1]
    wavfile = wavfile.split('.')[0]
    file_name = wavfile + '_new.wav'
    new_signal = librosa.resample(signal, sr, new_sample_rate) # 
    #librosa.output.write_wav(file_name, new_signal , new_sample_rate) 
    sf.write(file_name, new_signal, new_sample_rate, subtype='PCM_24')
    print(f'{file_name} has download.')
    return file_name

# 测试语音
# from paddlespeech.cli.asr.infer import ASRExecutor
# from paddlespeech.cli.tts.infer import TTSExecutor

# asr = ASRExecutor()
# tts = TTSExecutor()

# out_file =tts('你好呀，我是语音测试')
# print('tts',out_file)

# audio_file=resample_rate(out_file)
# result = asr(audio_file=audio_file)
# print('asr',result)

# import fastdeploy.vision as vision
# !pip install --upgrade paddlenlp
# import ppdiffusers
# help(ppdiffusers)
# from ppdiffusers import FastDeployStableDiffusionPipeline
# sd = FastDeployStableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5@fastdeploy",requires_safety_checker=False)

# 唇形
# !pip install --upgrade ppgan
# from ppgan.apps.wav2lip_predictor import Wav2LipPredictor

# wav2lip_predictor = Wav2LipPredictor(face_det_batch_size = 2,wav2lip_batch_size = 16,face_enhancement = True)
# wav2lip_predictor.run("test.png", 'output.wav', 'res.mp4')

from paddlespeech.cli.asr.infer import ASRExecutor
from paddlespeech.cli.tts.infer import TTSExecutor
from paddlenlp import Taskflow
from ppgan.apps.wav2lip_predictor import Wav2LipPredictor
from ppgan.apps.first_order_predictor import FirstOrderPredictor
from ppdiffusers import StableDiffusionPipeline

# 可选模型权重
# CompVis/stable-diffusion-v1-4
# runwayml/stable-diffusion-v1-5
# stabilityai/stable-diffusion-2-base （原始策略 512x512）
# stabilityai/stable-diffusion-2 （v-objective 768x768）
# Linaqruf/anything-v3.0
# ......


# from paddlenlp import Taskflow
# text_to_image = Taskflow("text_to_image")

# # https://github.com/JiehangXie/PaddleBoBo/blob/0.1/PaddleTools/GAN.py
# sd = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5",requires_safety_checker=False)
# sd = FastDeployStableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5@fastdeploy")

asr = ASRExecutor()
tts = TTSExecutor()
dialogue = Taskflow("dialogue")
wav2lip_predictor = Wav2LipPredictor(face_det_batch_size = 2,wav2lip_batch_size = 16,face_enhancement = True)

import base64,os
import librosa
import soundfile as sf

import hashlib

def get_prompt_id(text):
  m = hashlib.md5()
  m.update(text.encode("utf8"))
  return m.hexdigest()


def resample_rate(path,new_sample_rate = 16000):
    signal, sr = librosa.load(path, sr=None)
    wavfile = path.split('/')[-1]
    wavfile = wavfile.split('.')[0]
    file_name = wavfile + '_new.wav'
    new_signal = librosa.resample(signal, sr, new_sample_rate) # 
    #librosa.output.write_wav(file_name, new_signal , new_sample_rate) 
    sf.write(file_name, new_signal, new_sample_rate, subtype='PCM_24')
    print(f'{file_name} has download.')
    return file_name

# def text2img(text):
#   sd = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5",requires_safety_checker=False)
#   prompt = "a portrait of "+text+".passport photo， intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing "
#   image = sd(prompt, 
#         guidance_scale=7.5,
#         num_inference_steps=21,
#         height=416, width=384).images[0]
#   sd=None
#   return {
#       "image":image,
#       "prompt":prompt,
#       "id":get_prompt_id(prompt)
#   }


def audio2text(input_file):
  input_file=resample_rate(input_file,new_sample_rate = 16000)
  result = asr(audio_file=input_file,device='cpu')
  print('audio2text')
  return result

def text2audio(text):
  out_file =tts(text,device='cpu')
  print('text2audio')
  return out_file

def reply(t):
  data = [t]
  result = dialogue(data)
  print('reply',result[0])
  return result[0]

def wav2lip(input_video,input_audio):
  out_file='video.mp4'
  wav2lip_predictor.run(input_video, input_audio, out_file)
  return out_file


def FOM(source_image,driving_video,output_path):
  output,filename = os.path.split(output_path)
  first_order_predictor = FirstOrderPredictor(output = output,filename = filename, 
                              face_enhancement = True, 
                              ratio = 0.4,
                              relative = True,
                              image_size=512,
                              adapt_scale = True)
  first_order_predictor.run(source_image, driving_video)
  return os.path.join(output,filename)

def write_wav(data, samplerate,wav_file):
  # data, samplerate = sf.read('existing_file.wav')
  sf.write(wav_file, data, samplerate)
  return wav_file



#判断人脸
import face_recognition

def check_face_image(image_path):
  image = face_recognition.load_image_file(image_path)
  face_locations = face_recognition.face_locations(image)
  print("I found {} face(s) in this photograph.".format(len(face_locations)))

  if len(face_locations)==1:
    # top, right, bottom, left = face_locations[0]
    # width=right-left
    # height=bottom-top
    # s=(width*height)/(image.width*image.height)
    # if s>0.4:
    return True
  return False


# from fastapi import FastAPI, APIRouter,Body
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import HTMLResponse

# app = FastAPI()
# api_router = APIRouter()

# origins = ['*']

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=origins,
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# 调试用 GUI 
import os
import gradio as gr

# from google.colab import drive

# 挂载网盘
# drive.mount('/content/drive/')  
# # 切换路径
# root_path='/content/drive/MyDrive/data'
# os.chdir(root_path)

root_path='./data'
os.chdir(root_path)

portrait_video=None

def create_avatar(portrait_file):
  # avatar=text2img(text)
  #存下来，并索引
  # portrait_file=avatar['id']+'.png'
  # avatar['image'].save(portrait_file)
  # if check_face_image(portrait_file)==False:
  #     return portrait_file
  portrait_video=FOM(portrait_file,'driving_video.mp4','./portrait_video.mp4')
  return portrait_video


def test(text,wav_file_input):
  portrait_video='./portrait_video.mp4'
  if wav_file_input!=None:
    wav_file=write_wav(wav_file_input[1],wav_file_input[0],'./wav_file.wav')
    text=audio2text(wav_file)
  
  q=reply(text)
  input_audio=text2audio(q)
  result=wav2lip(portrait_video,input_audio)
  return result

# def talking_step_by_step(wav_file_input,portrait_video):
#     wav_file=write_wav(wav_file_input[1],wav_file_input[0],'./wav_file.wav')
#     text=audio2text(wav_file)
#     q=reply(text)
#     input_audio=text2audio(q)

#     # portrait_video=portrait
#     result=wav2lip(portrait_video,input_audio)
#     # os.path.join(os.path.dirname(__file__),  "video.mp4")
#     print(result)
#     return result




with gr.Blocks() as demo:
    with gr.Column():
        # avatar
        # input_text=gr.Textbox()
        input_i=gr.Image(type='filepath')
        output_video1=gr.Video()
        btn = gr.Button(value="创建形象")
        btn.click(create_avatar, inputs=[input_i], outputs=[output_video1])
    with gr.Column():
        # talking
        input_audio=gr.Audio(label="录音",type="numpy",source='microphone')
        input_talk_text=gr.Textbox()
        output_video2=gr.Video()
        btn2 = gr.Button(value="生成视频")
        btn2.click(test, inputs=[input_talk_text,input_audio], outputs=[output_video2])
      
    # gr.Interface(fn=create_avatar, inputs=[create_avatar], outputs=[output_video1],
    #       layout="vertical")
    # gr.Interface(fn=test, inputs=[input_audio,input_dropdown], outputs=[output_video2],
    #       layout="vertical")

    demo.queue(concurrency_count=2)

    demo.launch(
                # server_name='0.0.0.0',
                share=False,
                debug=True,
                max_threads=5,
                show_error=True,
                # file_directories=[root_path]
                )